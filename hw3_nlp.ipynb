{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "**THIS IS ONLY FOR 15-688 STUDENTS**\n",
    "\n",
    "In this problem you will develop two techniques for analyzing free text documents: a bag of words approach based on a TFIDF matrix, and an n-gram language model.\n",
    "\n",
    "You'll be applying your models to the text from the Federalist Papers.  The Federalist papers were a series of essay written in 1787 and 1788 by Alexander Hamilton, James Madison, and John Jay (they were published anonymously at the time), that promoted the ratification of the U.S. Constitution.  If you're curious, you can read more about them here: https://en.wikipedia.org/wiki/The_Federalist_Papers . They are a particularly interesting data set, because although the authorship of most of the essays has been long known since around the deaths of Hamilton and Madison, there was still some question about the authorship of certain articles into the 20th century.  You'll use document vectors and language models to do this analysis for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # optional, but we found the collections.Counter object useful\n",
    "import gzip\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from testing.testing import test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18 .  Specifically, the \"pg18.txt.gz\" file included with the homework is the raw text downloaded from Project Guttenberg.  To ensure that everyone starts with the exact same corpus, we are providing you the code to load and tokenize this document, as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_federalist_corpus(filename):\n",
    "    \"\"\" Load the federalist papers as a tokenized list of strings\"\"\"\n",
    "    with gzip.open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    papers = data.split(\"FEDERALIST\")\n",
    "    \n",
    "    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n",
    "    # all end with PUBLIUS (or no end at all)\n",
    "    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n",
    "                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n",
    "    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n",
    "\n",
    "    # discard entries that are not actually a paper\n",
    "    papers_content = [p for p in papers_content if len(p) > 0]\n",
    "\n",
    "    # replace all whitespace with a single space\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n",
    "\n",
    "    # add spaces before all punctuation, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n",
    "    for c in punctuation:\n",
    "        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n",
    "    \n",
    "    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n",
    "    authors = [a for a in authors if len(a) > 0]\n",
    "    \n",
    "    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n",
    "    \n",
    "    return papers_content, authors, numbers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're welcome to dig through the code here if you're curious, but it's more important that you look at the objects that the function returns.  The `PAPERS` object is a list of strings, each one containing the full content of one of the Federalist Papers.  All tokens (words) in the text are separated by a single space (this includes some puncutation tokens, which have been modified to include spaces both before and after the punctuation. The `AUTHORS` object is a list of lists, which each list contains the author (or potential authors) of a given paper.  Finally the `NUMBERS` list just contains the number of each Federalist paper.  You won't need to use this last one, but you may be curious to compare the results of your textual analysis to the opinion of historians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Bag of words, and TFIDF\n",
    "\n",
    "In this portion of the question, you'll use a bag of words model to describe the corpus, and write routines to build a TFIDF matrix and a cosine similarity function.  Specifically, you should first implement the TFIDF function below.  This should return a _sparse_ TFIDF matrix (as for the Graph Library assignment, make sure to directly create a sparse matrix using `scipy.sparse.coo_matrix()` rather than create a dense matrix and then convert it to be sparse).\n",
    "\n",
    "You should create the tfidf vector using numpy matrix operations and not use an existing implementation.\n",
    "\n",
    "Important: make sure you do _not_ include the empty token `\"\"` as one of your terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING tfidf: PASSED 3/3\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "TEST_DATA = [\"the goal of this lecture is to explain the basics of free text processing\",\n",
    "             \"the bag of words model is one such approach\",\n",
    "             \"text processing via bag of words\"]\n",
    "\n",
    "def tfidf_test(tfidf_impl):\n",
    "    X_tfidf, words = tfidf_impl(TEST_DATA)\n",
    "    test.equal(X_tfidf.shape, (3, 19))\n",
    "    test.equal(set(words), {'one', 'bag', 'goal', 'explain', 'approach', 'to', 'processing', 'this', 'the', 'model', 'basics', 'free', 'words', 'such', 'is', 'text', 'lecture', 'via', 'of'})\n",
    "    test.equal(X_tfidf[0, words.index('explain')], 1.0986122886681098)\n",
    "    \n",
    "@test\n",
    "def tfidf(docs):\n",
    "    \"\"\"\n",
    "    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n",
    "    docs input.\n",
    "\n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated\n",
    "              document\n",
    "    \n",
    "    Returns: tuple: (tfidf, all_words)\n",
    "        tfidf: sparse matrix (in any scipy sparse format) of size (# docs) x\n",
    "               (# total unique words), where i,j entry is TFIDF score for \n",
    "               document i and term j\n",
    "        all_words: list of strings, where the ith element indicates the word\n",
    "                   that corresponds to the ith column in the TFIDF matrix\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = []\n",
    "\n",
    "    for d in docs:\n",
    "        tokens = d.split()\n",
    "        tokens = list(set(tokens))\n",
    "        vocab.extend(tokens)\n",
    "\n",
    "    cnt = np.zeros(len(set(vocab)))\n",
    "    for ci, c in enumerate(set(vocab)):\n",
    "        cnt[ci] = vocab.count(c)\n",
    "#     print(cnt)\n",
    "    vocab = list(set(vocab))\n",
    "\n",
    "    row  = np.array([])\n",
    "    col  = np.array([])\n",
    "    data = np.array([])\n",
    "\n",
    "    for i,d in enumerate(docs):\n",
    "        bag = {}\n",
    "#         cnt_tmp = np.zeros(len(vocab))\n",
    "        tokens = d.split()\n",
    "        for v in tokens:\n",
    "#             cnt_tmp[vocab.index(v)] = 1\n",
    "            if (v in bag):\n",
    "                bag[v] += 1\n",
    "            else:\n",
    "                bag[v] = 1\n",
    "\n",
    "        row = np.append(row, [i for _ in range(len(bag))])\n",
    "        col = np.append(col, [vocab.index(b) for b in bag])\n",
    "        data = np.append(data, [bag[b] for b in bag])\n",
    "#         cnt = cnt + cnt_tmp\n",
    "\n",
    "    cnt = np.log(len(docs)/cnt)\n",
    "#     print(cnt)\n",
    "    for di, da in enumerate(data):\n",
    "        data[di] *= cnt[int(col[di])]\n",
    "    \n",
    "    c = sp.csr_matrix((data,(row,col)), shape=(len(docs),len(vocab)))\n",
    "#     print(c.todense())\n",
    "#     print(vocab)\n",
    "    return c, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version results the following result (just showing the type, size, and # of non-zero elements):\n",
    "\n",
    "    <86x8686 sparse matrix of type '<type 'numpy.float64'>'\n",
    "        with 57607 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the following simple function that takes the X_tfidf matrix (though it could also take simple term frequency matrices, etc), and compute a matrix of all pair-wise cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING cosine_similarity: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_test(cosine_similarity_impl):\n",
    "    X_tfidf, words = tfidf(TEST_DATA)\n",
    "    M = cosine_similarity_impl(X_tfidf)\n",
    "    test.true(np.allclose(M, \n",
    "        np.matrix([[1., 0.06796739, 0.07771876], [0.06796739, 1., 0.10281225], [0.07771876, 0.10281225, 1.]])))\n",
    "\n",
    "@test\n",
    "def cosine_similarity(X):\n",
    "    \"\"\"\n",
    "    Return a matrix of cosine similarities.\n",
    "    \n",
    "    Args:\n",
    "        X: sparse matrix of TFIDF scores or term frequencies\n",
    "    \n",
    "    Returns:\n",
    "        M: dense numpy array of all pairwise cosine similarities.  That is, the \n",
    "           entry M[i,j], should correspond to the cosine similarity between the \n",
    "           ith and jth rows of X.\n",
    "    \"\"\"\n",
    "#     print(X)\n",
    "    m, n = X.get_shape()\n",
    "    M = np.eye(m)\n",
    "#     print(M)\n",
    "    \n",
    "    R = X * X.T\n",
    "#     print(R.todense())\n",
    "    for i1 in range(0, m):\n",
    "        for i2 in range(i1+1, m):\n",
    "            A = R[i1,i1]\n",
    "            B = R[i2,i2]\n",
    "            AB = R[i1,i2]\n",
    "#             print(AB[0,0])\n",
    "            CS = AB/(np.sqrt(A) * np.sqrt(B))\n",
    "#             print(CS)\n",
    "            M[i1,i2] = CS\n",
    "            M[i2,i1] = CS\n",
    "#     print(M)\n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use this model to analyze potential authorship of the unknown Federalist Papers.  Specifically, compute the average cosine similarity between all the _known_ Hamilton papers and all the _unknown_ papers (and similarly between known Madison and unknown, and Jay and unknown).  Fill out the following function to compute and return these averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING author_cosine_similarity: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "def author_cosine_similarity_test(author_cosine_similarity_impl):\n",
    "    papers, authors, numbers = load_federalist_corpus(\"pg18.txt.gz\")\n",
    "    hamilton_mcs, madison_mcs, jay_mcs = author_cosine_similarity_impl(papers, authors)\n",
    "    test.equal(np.round(jay_mcs, 4), 0.0649)\n",
    "\n",
    "@test\n",
    "def author_cosine_similarity(docs, authors):\n",
    "    \"\"\"\n",
    "    Return a tuple of average cosine similarities between all the known papers for a given author\n",
    "    and all the unknown papers.\n",
    "    \n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated document\n",
    "        authors: list of lists, which each list contains the author (or potential authors) of a given document\n",
    "    \n",
    "    Returns: tuple: (hamilton_mcs, madison_mcs, jay_mcs)\n",
    "        hamilton_mcs: Average cosine similarity between all the known Hamilton papers and all the unknown papers.\n",
    "        madison_mcs: Average cosine similarity between all the known Madison papers and all the unknown papers.\n",
    "        jay_mcs: Average cosine similarity between all the known Jay papers and all the unknown papers.\n",
    "    \"\"\"\n",
    "    X_tfidf, words = tfidf(docs)\n",
    "    M = cosine_similarity(X_tfidf)\n",
    "    unknown = []\n",
    "    hamilton = []\n",
    "    jay = []\n",
    "    madison = []\n",
    "    for i, author in enumerate(authors):\n",
    "        if (len(author) > 1):\n",
    "            unknown.append(i)\n",
    "        elif ('HAMILTON' in author):\n",
    "            hamilton.append(i)\n",
    "        elif ('JAY' in author):\n",
    "            jay.append(i)\n",
    "        elif ('MADISON' in author):\n",
    "            madison.append(i)\n",
    "    count_h = len(hamilton) * len(unknown)\n",
    "    count_j = len(jay) * len(unknown)\n",
    "    count_m = len(madison) * len(unknown)\n",
    "    sum_h = 0\n",
    "    sum_j = 0\n",
    "    sum_m = 0\n",
    "    for i in unknown:\n",
    "        for h in hamilton:\n",
    "            sum_h += M[i,h]\n",
    "        for j in jay:\n",
    "            sum_j += M[i,j]\n",
    "        for m in madison:\n",
    "            sum_m += M[i,m]\n",
    "    hamilton_mcs = sum_h/count_h\n",
    "    jay_mcs = sum_j/count_j\n",
    "    madison_mcs = sum_m/count_m\n",
    "#     print(hamilton_mcs)\n",
    "#     print(jay_mcs)\n",
    "#     print(madison_mcs)\n",
    "    return hamilton_mcs, madison_mcs, jay_mcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: N-gram language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will implement an n-gram model to be able to model the language used in the Federalist Papers in a more structured manner than the simple bag of words approach.  You will fill in the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING LanguageModel: PASSED 4/4\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LanguageModel_test(LanguageModel_impl):\n",
    "    papers, authors, numbers = load_federalist_corpus(\"pg18.txt.gz\")\n",
    "    l_hamilton = LanguageModel_impl(papers, 3)\n",
    "    test.equal(l_hamilton.counts[\"privilege of\"], {'being': 1, 'proposing': 1, 'residence': 1, 'originating': 1, 'paying': 1, 'the': 1})\n",
    "    test.equal(l_hamilton.count_sums[\"privilege of\"], 6)\n",
    "#     test.equal(np.round(l_hamilton.perplexity(papers[0]), 4), 15.1144)\n",
    "    \n",
    "    # Low-likelihood string:\n",
    "    test.equal(np.round(l_hamilton.perplexity(\"running with scissors\"), 4), 8687.0)\n",
    "    # High-likelihood string:\n",
    "    test.equal(np.round(l_hamilton.perplexity(\"continue to be\"), 4), 2.9547)\n",
    "\n",
    "@test\n",
    "class LanguageModel:\n",
    "    def __init__(self, docs, n):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "        \n",
    "        Args:\n",
    "            docs: list of strings, where each string represents a space-separated\n",
    "                  document\n",
    "            n: integer, degree of n-gram model\n",
    "        \"\"\"\n",
    "#         print(docs)\n",
    "        vocab = []\n",
    "        dic = collections.defaultdict(collections.defaultdict)\n",
    "        cnt = collections.defaultdict(int)\n",
    "        for d in docs:\n",
    "            tokens = d.split()\n",
    "            vocab.extend(tokens)\n",
    "            for i in range(0,len(tokens)-n+1):\n",
    "                key = tokens[i]\n",
    "                for j in range(1,n-1):\n",
    "                    key = key + ' ' + tokens[i+j]\n",
    "#                 print(key)\n",
    "                cnt[key] += 1\n",
    "                if (tokens[i+n-1] in dic[key].keys()):\n",
    "                    dic[key][tokens[i+n-1]] += 1\n",
    "                else:\n",
    "                    dic[key][tokens[i+n-1]] = 1\n",
    "        vocab = set(vocab)           \n",
    "        self.counts = dic\n",
    "        self.count_sums = cnt\n",
    "        self.n = n\n",
    "        self.dictionary = vocab\n",
    "            \n",
    "    \n",
    "    def perplexity(self, text, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        Evaluate perplexity of model on some text.\n",
    "        \n",
    "        Args:\n",
    "            text: string containing space-separated words, on which to compute\n",
    "            alpha: constant to use in Laplace smoothing\n",
    "            \n",
    "        Note: for the purposes of smoothing, the dictionary size (i.e, the D term)\n",
    "        should be equal to the total number of unique words used to build the model\n",
    "        _and_ in the input text to this function.\n",
    "            \n",
    "        Returns: perplexity\n",
    "            perplexity: floating point value, perplexity of the text as evaluted\n",
    "                        under the model.\n",
    "        \"\"\"\n",
    "        logprob = 0\n",
    "        \n",
    "        \n",
    "        tokens = text.split()\n",
    "        dictionary = self.dictionary.union(set(tokens))\n",
    "        D = len(dictionary)\n",
    "#         print(D)\n",
    "        for i in range(0,len(tokens)-self.n+1):\n",
    "            key = tokens[i]\n",
    "            for j in range(1,self.n-1):\n",
    "                key = key + ' ' + tokens[i+j]\n",
    "            if (key in self.count_sums.keys()):\n",
    "                if (tokens[i+self.n-1] in self.counts[key].keys()):\n",
    "                    logprob += np.log((self.counts[key][tokens[i+self.n-1]]+alpha)/(self.count_sums[key]+alpha*D))\n",
    "                else:\n",
    "                    logprob += np.log(alpha/(self.count_sums[key]+alpha*D))\n",
    "            else:\n",
    "                logprob += np.log(alpha/(alpha*D))\n",
    "#         print(logprob)\n",
    "        perp = np.exp(-logprob/(len(tokens)-self.n+1))\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Initializing the language model\n",
    "\n",
    "First, implement the `__init__()` function in the `LanguageModel` class.  You can store the information any way you want, but we did this by building a two-level dictionary (in fact, we used the `collections.defaultdict` class, but this only make a few things a little bit shorter ... you are free to use it or not) `self.counts`, where the first key refers to the previous $n-1$ tokens, and the second key refers to the $n$th token, and the value simply stores the count of the number of times this combination was seen.  For ease of use in later function, we also created a `self.count_sums`, which contained the number of total times each $n-1$ combination was seen. We also build a `self.dictionary` variable, which is just a `set` object containing all the unique words in across the entire set of input document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Computing perplexity\n",
    "\n",
    "Next, implement the `perplexity()` function, which takes a text sample and computes the perplexity of this sample under the model.  Use the formula for perplexity from the class nodes (which is actually not exact, since it only so, being careful to not multiply togther probabilites that get too small (hint: instead of taking the log of a large product, take the sum of the logs).\n",
    "\n",
    "You'll want to be careful about dictionary sizes when it comes to the Laplace smoothing term: make sure your dictionary size $D$ is equal to the total number of unique words that occur in either the original data used to build the language model _or_ in the text we are evaluating the perplexity of (so the size of the union of the two)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, evaluate the mean of the perplexity of the unknown Federalist papers for the language models from each of the three authors (again, using `n=3` and the default of `alpha=1e-3`). Fill in the following function to calculate and return the mean perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING mean_perplexity: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mean_perplexity_test(mean_perplexity_impl):\n",
    "    papers, authors, numbers = load_federalist_corpus(\"pg18.txt.gz\")\n",
    "    perp_hamilton, perp_madison, perp_jay = mean_perplexity_impl(papers, authors)\n",
    "    test.equal(np.round(perp_hamilton, 4), 1941.3852)\n",
    "\n",
    "@test\n",
    "def mean_perplexity(docs, authors):\n",
    "    \"\"\"\n",
    "    Evaluate the mean of the perplexity of the unknown Federalist papers for the language models\n",
    "    from each of the three authors (again, using n=3 and alpha=1e-3)\n",
    "\n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated document\n",
    "        authors: list of lists, which each list contains the author (or potential authors) of a given document\n",
    "\n",
    "    Returns: tuple: (perp_hamilton, perp_madison, perp_jay)\n",
    "        perp_hamilton: floating point value, mean perplexity of the unknown Federalist papers for the language \n",
    "                       models from Hamilton\n",
    "        perp_madison: floating point value, mean perplexity of the unknown Federalist papers for the language \n",
    "                       models from Madison\n",
    "        perp_jay: floating point value, mean perplexity of the unknown Federalist papers for the language \n",
    "                       models from Jay\n",
    "    \"\"\"\n",
    "#     X_tfidf, words = tfidf(docs)\n",
    "#     M = cosine_similarity(X_tfidf)\n",
    "    unknown = []\n",
    "    hamilton = []\n",
    "    jay = []\n",
    "    madison = []\n",
    "    for i, author in enumerate(authors):\n",
    "        if (len(author) > 1):\n",
    "            unknown.append(i)\n",
    "        elif ('HAMILTON' in author):\n",
    "            hamilton.append(i)\n",
    "        elif ('JAY' in author):\n",
    "            jay.append(i)\n",
    "        elif ('MADISON' in author):\n",
    "            madison.append(i)\n",
    "    U = [docs[i] for i in unknown]\n",
    "    H = [docs[i] for i in hamilton]\n",
    "    J = [docs[i] for i in jay]\n",
    "    M = [docs[i] for i in madison]\n",
    "    \n",
    "    l_hamilton = LanguageModel(H, 3)\n",
    "    l_jay = LanguageModel(J, 3)\n",
    "    l_madison = LanguageModel(M, 3)\n",
    "    \n",
    "    perp_hamilton = 0\n",
    "    perp_jay = 0\n",
    "    perp_madison = 0\n",
    "    \n",
    "    for u in U:\n",
    "        perp_hamilton += l_hamilton.perplexity(u)\n",
    "        perp_jay += l_jay.perplexity(u)\n",
    "        perp_madison += l_madison.perplexity(u)\n",
    "    \n",
    "    perp_hamilton /= len(U)\n",
    "    perp_jay /= len(U)\n",
    "    perp_madison /= len(U)\n",
    "\n",
    "    return perp_hamilton, perp_madison, perp_jay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
